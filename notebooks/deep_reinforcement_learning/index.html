---
layout: content
metadata: notebooks_deep_reinforcement_learning_metadata
colab: https://colab.research.google.com/github/sut-ai/supplementary/blob/master/notebooks/deep_reinforcement_learning/index.ipynb
---

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Table-of-Contents">Table of Contents<a class="anchor-link" href="#Table-of-Contents">¶</a></h3><ul>
<li><a href="#Introduction">What is Deep Reinforcement Learning?</a></li>
<li><a href="#PlayingAtari">How Good is DRL?</a></li>
<li><a href="#Disadvantages">Disadvantages of DRL</a></li>
<li><a href="#Pong">Pong Game with DQN</a></li>
<li><a href="#References">Other useful links</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">¶</a></h1><p>Deep Reinforcement Learning (DRL) consists of Deep Learning (DL) and Reinforcement Learning. In past years, DL has created a new way to think about machine learning projects. Image Processing, NLP, and autonomous driving are successful examples of DL applications, and the idea of using DL in RL started immediately after the significant performance of neural networks in supervised learning. Therefore, some novel algorithms were developed, whose simpler versions had existed before. But, new ones are armed with neural networks.</p>
<p>However, the important point is to completely realize why DL is useful in RL. It is necessary to explain problems that are not feasible to solve without DL, and then discuss the effect and result of DL. Here there are some cases where DL helps us to efficiently and simply solve RL problems.</p>
<ul>
<li>
For finding a good (not the optimal) policy in a complex environment, it is necessary to have features to represent the state. Also, some features are more effective, and engineering of these features has to be done by an expert. For example, if we want to train an agent to play backgammon, an expert should design hand-made features about the backgammon state for our agent. In addition, if the environment is more complex than backgammon, it is more complex to design features. There should be a solution for complex problems that don't depend on a person's skill.
    </li>
<li>
There are many RL problems, and it is time-consuming to develop a specific algorithm for each of them. It seems a general solution for all problems of one kind is missing.
    </li>
<li>
In most of the real RL problems, state and action spaces are not discrete or finite anymore. If one wants to estimate V-Values or Q-values to do an algorithm-like policy iteration, it is necessary to have a model which can learn V-Values or Q-Values for the whole space from a finite dataset. Neural Networks are so capable to learn and predict an unknown function by knowing a little information about that. Hence, it may be a clever decision to use NNs to model the environments.
    </li>
<li>
An RL problem consists of some steps, and it is not easy to check whether each step is correctly working. It would be great if one application perform the all steps together.
    </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="title" src="/supplementary/assets/images/robotic.jpg"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PlayingAtari">PlayingAtari<a class="anchor-link" href="#PlayingAtari">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the pioneer papers in DRL is <a href="https://arxiv.org/format/1312.5602">"Playing Atari with Deep Reinforcement Learning"</a> which was published in 2013. This paper shows that it is possible to learn to play Atari games without having any experience with the game. In addition, the paper uses one model for different games, and it explains how this procedure can be done. Here, there is a summary of this paper.<br/>
<br/>
<br/>
        Let's assume that each frame of one Attari game is represented by an 210 <em> 160 RGB image, and there are 60 frames per second. The goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible. The network was not provided with any game-specific information or hand-designed visual features, and was not privy to the internal state of the emulator;
        <br/>
<br>
We consider tasks in which an agent interacts with an environment $E$, in this case the Atari emulator,
in a sequence of actions, observations and rewards. At each time-step the agent selects an action
at from the set of legal game actions, $A = \{1, . . . , K\}$. The action is passed to the emulator and
modifies its internal state and the game score. In general $E$ may be stochastic. The emulator’s
internal state is not observed by the agent; instead it observes an image $x^t \in \mathbb{R}^d$
from the emulator,
which is a vector of raw pixel values representing the current screen. In addition it receives a reward
$r_t$ representing the change in game score. Note that in general the game score may depend on the
whole prior sequence of actions and observations; feedback about an action may only be received
after many thousands of time-steps have elapsed.
    <br/>
<br/>
After defining states and actions, the agent starts interacting with $E$. The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards. We define the optimal action-value function $Q^∗
(s, a)$
as the maximum expected return achievable by following any strategy, after seeing some sequence
$s$ and then taking some action $a$, $Q^∗(s, a) = \max_{\pi} E [R_t|s_t = s, a_t = a, \pi]$, where $\pi$ is a policy
mapping sequences to actions (or distributions over actions). The optimal action-value function obeys an important identity known as the Bellman equation.<br/>
$$Q^∗(s, a) = E<em>{s'∼E} [r + \lambda \max</em>{a'}Q^</br></em>(s', a'))|s, a]$$</p>
<p>The basic idea behind many reinforcement learning algorithms is to estimate the actionvalue function, by using the Bellman equation as an iterative update. In practice, this basic approach is totally impractical,
because the action-value function is estimated separately for each sequence, without any generalisation. Instead, it is common to use a function approximator to estimate the action-value function, $Q(s, a; θ) \approx Q^∗(s, a)$. We use to a neural network function approximator with weights $\theta$ as a Q-network. We use an architecture
in which there is a separate output unit for each possible action, and only the state representation is
an input to the neural network. Also, our model outperforms all previous approaches on six of the games and surpasses
a human expert on three of them.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="title" src="/supplementary/assets/images/table1.jpg"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Disadvantages">Disadvantages<a class="anchor-link" href="#Disadvantages">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>RL projects are sometimes critical, and the algorithms must not be vulnerable to adversarial policies. Although attack and defenses mechanisms in RL are studied very well, the DRL algorithm can be misled by adversarial policies. There is a paper called <a href="https://arxiv.org/abs/1905.10615">ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING</a> that discusses problems with DRL algorithms. Here is a summary of this paper.
<br/>
<br/>
        Is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to
be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent.</p>
<p>We model the victim as playing against an opponent in a two-player Markov game (Shapley, 1953). Our threat model assumes the attacker can control the opponent, in which case we call the opponent an adversary. We denote the adversary and victim by subscript $\alpha$ and $\nu$ respectively. The adversary is allowed unlimited black-box access to actions sampled from $\pi_\nu$, but is not given any white-box information such as weights or activations. We further assume the victim agent follows a fixed stochastic policy $\pi_\nu$, corresponding to the common case of a pre-trained model deployed with
static weights.</p>
<p>Since the victim policy $\pi_\nu$ is held fixed, the two-player Markov game $M$ reduces to a single-player
MDP $M_\alpha = (S, A_\alpha, T_\alpha, R_\alpha)$ that the attacker must solve. The goal of the
attacker is to find an adversarial policy $\pi_\alpha$ maximizing the sum of discounted rewards:
$$\sum_{t=0}^{\infty}\lambda R_\alpha(s^{(t)}, a_\alpha^{(t)}, s_\alpha^{(t)}), \quad \text{where} \quad s^{(t+1)} ∼ T_\alpha(s_\alpha^{(t)}, a_\alpha^{(t)}) \quad and \quad a_α ∼ \pi_\alpha(· | s^{(t)}))$$
Note the MDP’s dynamics $T_\alpha$ will be unknown even if the Markov game’s dynamics $T$ are known
since the victim policy $\pi_\mu$ is a black-box. Consequently, the attacker must solve an RL problem.</p>
<p>The adversarial policies beat the victim not by performing the intended task
(e.g. blocking a goal), but rather by exploiting weaknesses in the victim’s policy. This effect is best
seen by watching the videos at <a href="https://adversarialpolicies.github.io/">https://adversarialpolicies.github.io/</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="title" src="/supplementary/assets/images/Games.png"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One might wonder if the adversarial policies win because they are outside the
training distribution of the victim. To test this, we evaluate victims against two simple off-distribution
baselines: a random policy Rand (green) and a lifeless policy Zero (red). These baselines win as
often as 30% to 50% in Kick and Defend, but less than 1% of the time in Sumo and You Shall Not
Pass. This is well below the performance of our adversarial policies. We conclude that most victim
policies are robust to off-distribution observations that are not adversarially optimized.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Pong">Pong<a class="anchor-link" href="#Pong">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Environment-and-Lib">Environment and Lib<a class="anchor-link" href="#Environment-and-Lib">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installation">Installation<a class="anchor-link" href="#Installation">¶</a></h2><p>Run these cells on Google Colab (they don't work on Windows!)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install torch
<span class="o">!</span>pip install gym
<span class="o">!</span>pip install atari_py
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because the main point of the project is implementing the DQN algorithm, so we won't build the game from scratch, we just use the environment provided by openAI, gym. The deep learning lib I use here is pytorch, because it's easy to learn and debug.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load toolkit</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">src.env_wrapper</span> <span class="kn">import</span> <span class="n">make_atari</span><span class="p">,</span> <span class="n">wrap_deepmind</span><span class="p">,</span> <span class="n">wrap_pytorch</span>

<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="c1"># if we have a gpu to make the compute faster</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s1">'http://www.atarimania.com/roms/Roms.rar'</span><span class="p">,</span><span class="s1">'Roms.rar'</span><span class="p">)</span>
<span class="o">!</span>pip install unrar
<span class="o">!</span>unrar x Roms.rar
<span class="o">!</span>mkdir rars
<span class="o">!</span>mv HC<span class="se">\ </span>ROMS.zip   rars
<span class="o">!</span>mv ROMS.zip  rars
<span class="o">!</span>python -m atari_py.import_roms rars
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Game-structure">Game structure<a class="anchor-link" href="#Game-structure">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load game environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'PongNoFrameskip-v4'</span><span class="p">)</span>
<span class="c1"># Reset environment</span>
<span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Pixel shape</span>
<span class="n">observation</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The observation is an image of size 210*160 with 3 channels (rgb).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we take a look at our action space.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">unwrapped</span><span class="o">.</span><span class="n">get_action_meanings</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although we have 6 discrete action values, there are only three real actions. 0&amp;1 stands for doing nothing. 2&amp;4 refers to going up. 3&amp;5 refers to going down.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># lets take some steps to see middle game scene in pong game,</span>
<span class="c1"># 0 is numeric action to stay at same place </span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="c1"># 0 means stay the same place(or do nothing)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 2 is numeric action to move paddle up in game</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">observation2</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="c1"># 2 means go up</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">observation2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># 3 is numeric action to move paddle down in game</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">observation3</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="c1"># 3 means go down</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">observation3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the game, we control the green paddle which is on the right side. When the ball passes our paddle and goes to end right, we get <strong>reward of -1</strong> for losing. If the ball crosses the opponent and reaches the left, we get a <strong>reward of +1</strong> for winning. The game finishes if one of the players reaches <strong>21 scores</strong>.</p>
<p>So the definition of the system in reinforcement learning method is clear.</p>
<p><strong>State</strong> is the screen of game.</p>
<p><strong>Action</strong> is going go up, go down and stay still.</p>
<p><strong>Goal: </strong>maximize total reward</p>
<p>We warp frames to <strong>84x84 images</strong> as done in the later work and rescale the pixel values.</p>
<p>Next we define a network policy with an architecture that inputs a 84x84 image and chooses an action.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Model-Structure">Model Structure<a class="anchor-link" href="#Model-Structure">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use three convolutional layers and two fully connected layers to construct the <strong>DQN model</strong>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Pong</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_num</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Pong</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Three convolutional layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># Two fully connected layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_size</span><span class="p">(),</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">action_num</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># evaluate the network and return the action values </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">feature_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Return the size of last convolutional layer</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">feature_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feature_num</span>

    <span class="k">def</span> <span class="nf">action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># Choose action based on max action values</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">USE_CUDA</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can define any network model as you want. Based on architecture and complexity of your model, it may train slow or fast, or even not train at all (too hard to train).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">Pong</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>  <span class="c1"># show the structure of the network</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Train">Train<a class="anchor-link" href="#Train">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Defining-loss">Defining loss<a class="anchor-link" href="#Defining-loss">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The loss is the squared loss: <strong>${(Q\_ real-Q\_ expected)}^{2}$</strong>, as the code below. We can use the loss function to calculate gradient and optimize our DQN model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="c1"># Train begin</span>
    <span class="c1"># Sample batch data</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
    
    <span class="c1"># Calculate loss</span>
    <span class="c1"># Input the data as tensors</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
    <span class="c1"># Use CUDA</span>
    <span class="k">if</span> <span class="n">USE_CUDA</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">next_state</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">done</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="c1"># Calculate Q values based on the model</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">next_q_values</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>

    <span class="n">q_value</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_q_value</span> <span class="o">=</span> <span class="n">next_q_values</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Set expected_q_value</span>
    <span class="n">expected_q_value</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q_value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
    <span class="c1"># Set loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_value</span> <span class="o">-</span> <span class="n">expected_q_value</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Clear grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Grad step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># Train end</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hyper-parameters">Hyper parameters<a class="anchor-link" href="#Hyper-parameters">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below are some hyper parameters and basic parameters that will be used in the training process.</p>
<p><strong>Batch_size:</strong> How many rounds we play before updating the weights of our network.</p>
<p><strong>Gamma:</strong> The discount factor we use to discount the effect of old actions on the final result.</p>
<p><strong>Epsilon_begin:</strong> Exploration rates at the beginning</p>
<p><strong>Epsilon_end:</strong> Exploration rates at the end</p>
<p><strong>Epsilon_decay:</strong> The speed of decreasing the exploration rates</p>
<p><strong>Replay_buffer_size:</strong> The maximum number of data that our buffer can hold. If the the length of the data is larger than this size, we should drop oldest data point to make room for new data.</p>
<p><strong>Replay_initial:</strong> The size of the first buffer before we begin to train from the buffer.</p>
<p><strong>Learning_rate:</strong> The rate at which we learn from our results to compute the new weights. A higher rate means we react more to results and a lower rate means we don’t react as strongly to each result.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="c1"># game environment id</span>
<span class="n">env_id</span> <span class="o">=</span> <span class="s1">'PongNoFrameskip-v4'</span>
<span class="c1"># env_id = 'Pong-v0'</span>

<span class="nb">round</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">save_every_n_round</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">render</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">one_round_max_frame</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="c1"># Adam optimizer learning rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="c1"># exploration rates at beginning and at the end</span>
<span class="n">epsilon_begin</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">epsilon_end</span> <span class="o">=</span> <span class="mf">0.02</span>

<span class="c1"># Set experience replay buffer size</span>
<span class="n">replay_buffer_size</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">replay_initial</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="c1"># Set batch size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="c1"># Set discount</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="c1"># Output the model</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="s2">"batch_"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">+</span><span class="s2">"_pong.model"</span>
<span class="n">file_handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">FileHandler</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">+</span><span class="s2">".log"</span><span class="p">)</span>
<span class="n">file_handler</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">file_handler</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">I</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-process">Training process<a class="anchor-link" href="#Training-process">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We’ll put everything in a while block for now but in reality you might set up a break condition to stop the process.</p>
<p>The first step to our algorithm is <strong>preprocessing</strong> the image of the game that OpenAI Gym passed to us.</p>
<p>In the env.wrapper code, we convert the image format from RGB to GREY because the color is not important to us and GREY format is easy to train. In addition, we resize the image from 210x160x3 to 1x84x84.</p>
<p>The following cell may run very long time , so just be patient, or use the pre trained model to test.
<strong>Uncomment the "break" to run.</strong></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Accepts a tuple (s,a,r,s') and keeps a list, returns a random batch of tuples as needed</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">replay_buffer_size</span><span class="p">)</span>
<span class="c1"># Set environment that skip over the 4 observations over all the time steps</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_atari</span><span class="p">(</span><span class="n">env_id</span><span class="p">)</span>
<span class="c1"># Configure environment for DeepMind-style Atari</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">wrap_deepmind</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="c1"># Set the environment that can be used in pytorch</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">wrap_pytorch</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Pong</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
<span class="k">if</span> <span class="n">USE_CUDA</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c1"># Train step</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="c1"># Set the initial exploration rate</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_begin</span>
<span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="c1"># Decay exploration from 1 to 0.02</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="n">epsilon_by_frame</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="n">epsilon_end</span> <span class="o">+</span> <span class="p">(</span><span class="n">epsilon_begin</span> <span class="o">-</span> <span class="n">epsilon_end</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">frame_idx</span> <span class="o">/</span> <span class="n">epsilon_decay</span><span class="p">)</span>
<span class="n">frame_seq</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">round_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># while True:</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># break</span>
    <span class="c1"># reset environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="c1"># state = pre_process(state)</span>
    <span class="c1"># state = np.concatenate([state, state])</span>

    <span class="n">round_over</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Runing until the game round over</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">round_over</span><span class="p">:</span>
        <span class="c1"># Set exploration rate</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_by_frame</span><span class="p">(</span><span class="n">frame_seq</span><span class="p">)</span>
        <span class="c1"># behave according to an epsilon greedy policy</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># agent takes the action, and the environment responds</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">round_over</span> <span class="o">=</span> <span class="n">done</span>
        <span class="c1"># next_state = pre_process(next_state)</span>
        <span class="c1"># next_state = np.concatenate([state[80:], next_state])</span>

        <span class="c1"># push to replay buffer</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="c1"># update the state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">frame_seq</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># print(frame_seq)</span>
        <span class="c1"># Gather the rewards</span>
        <span class="n">round_reward</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="c1"># Start training when there are enough memory exists</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">replay_initial</span><span class="p">:</span>
            <span class="n">train</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">frame_seq</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print("saving model", frame_seq)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"current round: "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">)</span><span class="o">+</span><span class="s2">"saving model"</span><span class="p">)</span>
            <span class="c1"># from datetime import datetime</span>
            <span class="c1"># start = datetime.now()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_file</span><span class="p">)</span>
            <span class="c1"># print(datetime.now() - start)</span>
    <span class="c1"># Caculate round number</span>
    <span class="nb">round</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># print("round over", round_reward)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"round reward: "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">round_reward</span><span class="p">))</span>
    <span class="n">round_reward</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results-of-training">Results of training<a class="anchor-link" href="#Results-of-training">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We write a script to save the model in the file and log rewards from time to time.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Read the log trail</span>
<span class="k">def</span> <span class="nf">read_log</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"round reward"</span><span class="p">):</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"</span><span class="se">\r</span><span class="s2">"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"round reward: "</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">reward</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Plot the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">read_log</span><span class="p">(</span><span class="s2">"32.log"</span><span class="p">))</span>  <span class="c1"># reward of the train process</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Round'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'total reward per round'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'DQN Pong game q-learning (training)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Interpretation">Interpretation<a class="anchor-link" href="#Interpretation">¶</a></h2><p>We can see that the agent has learned some tricks to defend and even win. From the above figures, we can figure out that at the beginning, the total rewards of the agent in one episode are around -20, which means the agent is losing the game. The reason is that with the relatively large epsilon, the agent is likely to explore more possibilities. However, with the increase of the number of game round, the rewards become larger. After around 1750 steps for batch size 32, the rewards turn positive and even reach 20, which means the agent begins to win. It can be explained that with the decay of the epsilon, the agent is prone to exploit rather than explore, leading to the higher rewards.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Test">Test<a class="anchor-link" href="#Test">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check out the model we trained to see whether it can beat the hard coded AI or not.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load the model</span>
<span class="n">pong</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"batch_32_pong.model"</span><span class="p">)</span>
<span class="c1"># Set test rounds</span>
<span class="n">test_rounds</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">win_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">current</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">round</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_rounds</span><span class="p">):</span>
    <span class="c1"># reset the environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">current</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">round_over</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Running until the round over</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">round_over</span><span class="p">:</span>
<span class="c1">#         env.render()</span>
        <span class="c1"># Running greedy </span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">pong</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># agent takes the action, and the environment responds</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">round_over</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="c1"># Caculate round reward</span>
        <span class="n">current</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="c1"># Update state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
    <span class="c1"># Calculate number of victory</span>
    <span class="k">if</span> <span class="n">reward</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">win_num</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
<span class="c1"># Plot the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Round'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'total reward per round'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'DQN Pong game q-learning (test)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">21</span><span class="p">,</span><span class="mi">21</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># Print the wining rate</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"we win at rate "</span><span class="p">,</span> <span class="n">win_num</span><span class="o">/</span><span class="n">test_rounds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Interpretation">Interpretation<a class="anchor-link" href="#Interpretation">¶</a></h2><p>Now, we can see that the total reward per round are all above 0, which means we win the game every time. However, the mean reward is around 15, which means we cannot hit the ball back every time.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results">Results<a class="anchor-link" href="#Results">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Let us view the game</span>
<span class="c1"># Play the mp4 file</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">video</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">'test.mp4'</span><span class="p">,</span> <span class="s1">'r+b'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">'''&lt;video alt="test" controls&gt;</span>
<span class="s1">                &lt;source src="data:video/mp4;base64,</span><span class="si">{0}</span><span class="s1">" type="video/mp4" /&gt;</span>
<span class="s1">             &lt;/video&gt;'''</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoded</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">'ascii'</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Limitations">Limitations<a class="anchor-link" href="#Limitations">¶</a></h1><p><strong>1. The training process is too long.</strong> With GPU, it takes us a whole day to train the model.</p>
<p><strong>2. The score result is not as high as we expected.</strong> The mean score of our model does not reach 21, which means the trained agent does not always hit the ball back every time.</p>
<h1 id="Next-steps">Next steps<a class="anchor-link" href="#Next-steps">¶</a></h1><p>Here are some ideas to improve the training process and agents:</p>
<p><strong>1.Generate simulations of batches in parallel. </strong> I tried using multithreading to generate and play batches of games at the same time to increase the speed of training process. The problem was that the backend library for the game had issues creating environments in multiple threads. Maybe you can overcome this by implementing a multiprocess method.</p>
<p><strong>2.Giving a reward to the agent for catching the ball could be good </strong>for not losing (for example a +0.5 reward). Now we are just giving positive reward to agent for scoring. If it catches the ball and then gets a -1 reward, it will think it is probably bad to catch the ball.</p>
<p><strong>3. Use a different gamma to train the model.</strong> In our model, we use 0.99 as gamma. We may try other values for the gamma parameter, for example, 0.9. This may improve the training speed.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
        Playing Atari with Deep Reinforcement Learning
    </li>
<li>
        ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING
    </li>
    Also in case you are interested in RL and DRL, these courses may help you.
    <li> <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Reinforcement Learning By David Silver</a></li>
<li> <a href="http://rail.eecs.berkeley.edu/deeprlcourse/"> Deep Reinforcement Learning By Sergey Levine</a></li>
<li> <a href="https://github.com/xs2315/Deep-reinforcement-learning-for-Pong-game"> Pong DQN </a> </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
