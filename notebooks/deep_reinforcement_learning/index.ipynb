{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [What is Deep Reinforcement Learning?](#Introduction)\n",
    "* [How Good is DRL?](#PlayingAtari)\n",
    "* [Disadvantages of DRL](#Disadvantages)\n",
    "* [Pong Game with DQN](#Pong)\n",
    "* [Other useful links](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Deep Reinforcement Learning (DRL) consists of Deep Learning (DL) and Reinforcement Learning. In past years, DL has created a new way to think about machine learning projects. Image Processing, NLP, and autonomous driving are successful examples of DL applications, and the idea of using DL in RL started immediately after the significant performance of neural networks in supervised learning. Therefore, some novel algorithms were developed, whose simpler versions had existed before. But, new ones are armed with neural networks.\n",
    "\n",
    "However, the important point is to completely realize why DL is useful in RL. It is necessary to explain problems that are not feasible to solve without DL, and then discuss the effect and result of DL. Here there are some cases where DL helps us to efficiently and simply solve RL problems.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "For finding a good (not the optimal) policy in a complex environment, it is necessary to have features to represent the state. Also, some features are more effective, and engineering of these features has to be done by an expert. For example, if we want to train an agent to play backgammon, an expert should design hand-made features about the backgammon state for our agent. In addition, if the environment is more complex than backgammon, it is more complex to design features. There should be a solution for complex problems that don't depend on a person's skill.\n",
    "    </li>\n",
    "    <li>\n",
    "There are many RL problems, and it is time-consuming to develop a specific algorithm for each of them. It seems a general solution for all problems of one kind is missing.\n",
    "    </li>\n",
    "    <li>\n",
    "In most of the real RL problems, state and action spaces are not discrete or finite anymore. If one wants to estimate V-Values or Q-values to do an algorithm-like policy iteration, it is necessary to have a model which can learn V-Values or Q-Values for the whole space from a finite dataset. Neural Networks are so capable to learn and predict an unknown function by knowing a little information about that. Hence, it may be a clever decision to use NNs to model the environments.\n",
    "    </li>\n",
    "    <li>\n",
    "An RL problem consists of some steps, and it is not easy to check whether each step is correctly working. It would be great if one application perform the all steps together.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/robotic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlayingAtari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the pioneer papers in DRL is <a href=\"https://arxiv.org/format/1312.5602\">\"Playing Atari with Deep Reinforcement Learning\"</a> which was published in 2013. This paper shows that it is possible to learn to play Atari games without having any experience with the game. In addition, the paper uses one model for different games, and it explains how this procedure can be done. Here, there is a summary of this paper.  \n",
    "        <br>\n",
    "        <br>\n",
    "        Let's assume that each frame of one Attari game is represented by an 210 * 160 RGB image, and there are 60 frames per second. The goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible. The network was not provided with any game-specific information or hand-designed visual features, and was not privy to the internal state of the emulator;\n",
    "        <br/>\n",
    "        <br>\n",
    "We consider tasks in which an agent interacts with an environment $E$, in this case the Atari emulator,\n",
    "in a sequence of actions, observations and rewards. At each time-step the agent selects an action\n",
    "at from the set of legal game actions, $A = \\{1, . . . , K\\}$. The action is passed to the emulator and\n",
    "modifies its internal state and the game score. In general $E$ may be stochastic. The emulator’s\n",
    "internal state is not observed by the agent; instead it observes an image $x^t \\in \\mathbb{R}^d$\n",
    "from the emulator,\n",
    "which is a vector of raw pixel values representing the current screen. In addition it receives a reward\n",
    "$r_t$ representing the change in game score. Note that in general the game score may depend on the\n",
    "whole prior sequence of actions and observations; feedback about an action may only be received\n",
    "after many thousands of time-steps have elapsed.\n",
    "    <br>\n",
    "    <br>\n",
    "After defining states and actions, the agent starts interacting with $E$. The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards. We define the optimal action-value function $Q^∗\n",
    "(s, a)$\n",
    "as the maximum expected return achievable by following any strategy, after seeing some sequence\n",
    "$s$ and then taking some action $a$, $Q^∗(s, a) = \\max_{\\pi} E [R_t|s_t = s, a_t = a, \\pi]$, where $\\pi$ is a policy\n",
    "mapping sequences to actions (or distributions over actions). The optimal action-value function obeys an important identity known as the Bellman equation.        \n",
    "$$Q^∗(s, a) = E_{s'∼E} [r + \\lambda \\max_{a'}Q^*(s', a'))|s, a]$$\n",
    "        \n",
    "The basic idea behind many reinforcement learning algorithms is to estimate the action\u0002value function, by using the Bellman equation as an iterative update. In practice, this basic approach is totally impractical,\n",
    "because the action-value function is estimated separately for each sequence, without any generalisation. Instead, it is common to use a function approximator to estimate the action-value function, $Q(s, a; θ) \\approx Q^∗(s, a)$. We use to a neural network function approximator with weights $\\theta$ as a Q-network. We use an architecture\n",
    "in which there is a separate output unit for each possible action, and only the state representation is\n",
    "an input to the neural network. Also, our model outperforms all previous approaches on six of the games and surpasses\n",
    "a human expert on three of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/table1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL projects are sometimes critical, and the algorithms must not be vulnerable to adversarial policies. Although attack and defenses mechanisms in RL are studied very well, the DRL algorithm can be misled by adversarial policies. There is a paper called <a href = \"https://arxiv.org/abs/1905.10615\">ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING</a> that discusses problems with DRL algorithms. Here is a summary of this paper.\n",
    "<br>\n",
    "<br/>\n",
    "        Is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to\n",
    "be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent.\n",
    "        \n",
    "We model the victim as playing against an opponent in a two-player Markov game (Shapley, 1953). Our threat model assumes the attacker can control the opponent, in which case we call the opponent an adversary. We denote the adversary and victim by subscript $\\alpha$ and $\\nu$ respectively. The adversary is allowed unlimited black-box access to actions sampled from $\\pi_\\nu$, but is not given any white-box information such as weights or activations. We further assume the victim agent follows a fixed stochastic policy $\\pi_\\nu$, corresponding to the common case of a pre-trained model deployed with\n",
    "static weights.\n",
    "\n",
    "Since the victim policy $\\pi_\\nu$ is held fixed, the two-player Markov game $M$ reduces to a single-player\n",
    "MDP $M_\\alpha = (S, A_\\alpha, T_\\alpha, R_\\alpha)$ that the attacker must solve. The goal of the\n",
    "attacker is to find an adversarial policy $\\pi_\\alpha$ maximizing the sum of discounted rewards:\n",
    "$$\\sum_{t=0}^{\\infty}\\lambda R_\\alpha(s^{(t)}, a_\\alpha^{(t)}, s_\\alpha^{(t)}), \\quad \\text{where} \\quad s^{(t+1)} ∼ T_\\alpha(s_\\alpha^{(t)}, a_\\alpha^{(t)}) \\quad and \\quad a_α ∼ \\pi_\\alpha(· | s^{(t)}))$$\n",
    "Note the MDP’s dynamics $T_\\alpha$ will be unknown even if the Markov game’s dynamics $T$ are known\n",
    "since the victim policy $\\pi_\\mu$ is a black-box. Consequently, the attacker must solve an RL problem.\n",
    "\n",
    "The adversarial policies beat the victim not by performing the intended task\n",
    "(e.g. blocking a goal), but rather by exploiting weaknesses in the victim’s policy. This effect is best\n",
    "seen by watching the videos at https://adversarialpolicies.github.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Games.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder if the adversarial policies win because they are outside the\n",
    "training distribution of the victim. To test this, we evaluate victims against two simple off-distribution\n",
    "baselines: a random policy Rand (green) and a lifeless policy Zero (red). These baselines win as\n",
    "often as 30% to 50% in Kick and Defend, but less than 1% of the time in Sumo and You Shall Not\n",
    "Pass. This is well below the performance of our adversarial policies. We conclude that most victim\n",
    "policies are robust to off-distribution observations that are not adversarially optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Run these cells on Google Colab (they don't work on Windows!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gym\n",
    "!pip install atari_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the main point of the project is implementing the DQN algorithm, so we won't build the game from scratch, we just use the environment provided by openAI, gym. The deep learning lib I use here is pytorch, because it's easy to learn and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load toolkit\n",
    "import logging\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.autograd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from src.env_wrapper import make_atari, wrap_deepmind, wrap_pytorch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # if we have a gpu to make the compute faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "!pip install unrar\n",
    "!unrar x Roms.rar\n",
    "!mkdir rars\n",
    "!mv HC\\ ROMS.zip   rars\n",
    "!mv ROMS.zip  rars\n",
    "!python -m atari_py.import_roms rars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Game structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load game environment\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "# Reset environment\n",
    "observation = env.reset()\n",
    "# Pixel shape\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation is an image of size 210*160 with 3 channels (rgb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take a look at our action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have 6 discrete action values, there are only three real actions. 0&1 stands for doing nothing. 2&4 refers to going up. 3&5 refers to going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take some steps to see middle game scene in pong game,\n",
    "# 0 is numeric action to stay at same place \n",
    "for i in range(20):\n",
    "    observation, reward, done, info = env.step(0)# 0 means stay the same place(or do nothing)\n",
    "    \n",
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 is numeric action to move paddle up in game\n",
    "for i in range(20):\n",
    "    observation2, reward, done, info = env.step(2)# 2 means go up\n",
    "    \n",
    "plt.imshow(observation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 is numeric action to move paddle down in game\n",
    "for i in range(20):\n",
    "    observation3, reward, done, info = env.step(3)# 3 means go down\n",
    "    \n",
    "plt.imshow(observation3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the game, we control the green paddle which is on the right side. When the ball passes our paddle and goes to end right, we get **reward of -1** for losing. If the ball crosses the opponent and reaches the left, we get a **reward of +1** for winning. The game finishes if one of the players reaches **21 scores**.\n",
    "\n",
    "So the definition of the system in reinforcement learning method is clear.\n",
    "\n",
    "**State** is the screen of game. \n",
    "\n",
    "**Action** is going go up, go down and stay still.\n",
    "\n",
    "**Goal: **maximize total reward\n",
    "\n",
    "We warp frames to **84x84 images** as done in the later work and rescale the pixel values.\n",
    "\n",
    "Next we define a network policy with an architecture that inputs a 84x84 image and chooses an action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three convolutional layers and two fully connected layers to construct the **DQN model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pong(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, action_num):\n",
    "        super(Pong, self).__init__()\n",
    "        # Three convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Two fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # evaluate the network and return the action values \n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        # Return the size of last convolutional layer\n",
    "        with torch.no_grad():\n",
    "            feature_num = self.conv(torch.zeros(1, 1, 84, 84)).view(1, -1).size(1)\n",
    "        return feature_num\n",
    "\n",
    "    def action(self, state):\n",
    "        # Choose action based on max action values\n",
    "        state = to_tensor(state)\n",
    "        if USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        q_values = self.forward(state)\n",
    "        return q_values.max(1)[1].data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define any network model as you want. Based on architecture and complexity of your model, it may train slow or fast, or even not train at all (too hard to train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pong(6)  # show the structure of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the squared loss: **${(Q\\_ real-Q\\_ expected)}^{2}$**, as the code below. We can use the loss function to calculate gradient and optimize our DQN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Train begin\n",
    "    # Sample batch data\n",
    "    state, action, reward, next_state, done = zip(*random.sample(replay_buffer, batch_size))\n",
    "    state = np.concatenate(state)\n",
    "    next_state = np.concatenate(next_state)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # Input the data as tensors\n",
    "    state = torch.Tensor(state)\n",
    "    next_state = torch.Tensor(next_state)\n",
    "    action = torch.LongTensor(action)\n",
    "    reward = torch.Tensor(reward)\n",
    "    done = torch.Tensor(done)\n",
    "    # Use CUDA\n",
    "    if USE_CUDA:\n",
    "        state = state.cuda()\n",
    "        next_state = next_state.cuda()\n",
    "        action = action.cuda()\n",
    "        reward = reward.cuda()\n",
    "        done = done.cuda()\n",
    "    # Calculate Q values based on the model\n",
    "    q_values = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_values.max(1)[0]\n",
    "    # Set expected_q_value\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    # Set loss\n",
    "    loss = (q_value - expected_q_value).pow(2).mean()\n",
    "\n",
    "    # Clear grad\n",
    "    optimizer.zero_grad()\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    # Grad step\n",
    "    optimizer.step()\n",
    "    # Train end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some hyper parameters and basic parameters that will be used in the training process.\n",
    "\n",
    "**Batch_size:** How many rounds we play before updating the weights of our network.\n",
    "\n",
    "**Gamma:** The discount factor we use to discount the effect of old actions on the final result. \n",
    "\n",
    "**Epsilon_begin:** Exploration rates at the beginning\n",
    "\n",
    "**Epsilon_end:** Exploration rates at the end\n",
    "\n",
    "**Epsilon_decay:** The speed of decreasing the exploration rates\n",
    "\n",
    "**Replay_buffer_size:** The maximum number of data that our buffer can hold. If the the length of the data is larger than this size, we should drop oldest data point to make room for new data.\n",
    "\n",
    "**Replay_initial:** The size of the first buffer before we begin to train from the buffer.\n",
    "\n",
    "**Learning_rate:** The rate at which we learn from our results to compute the new weights. A higher rate means we react more to results and a lower rate means we don’t react as strongly to each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "# game environment id\n",
    "env_id = 'PongNoFrameskip-v4'\n",
    "# env_id = 'Pong-v0'\n",
    "\n",
    "round = 0\n",
    "save_every_n_round = 100\n",
    "\n",
    "render = False\n",
    "\n",
    "one_round_max_frame = 10000\n",
    "# Adam optimizer learning rate\n",
    "learning_rate = 0.00001\n",
    "# exploration rates at beginning and at the end\n",
    "epsilon_begin = 1\n",
    "epsilon_end = 0.02\n",
    "\n",
    "# Set experience replay buffer size\n",
    "replay_buffer_size = 100000\n",
    "replay_initial = 10000\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "# Set discount\n",
    "gamma = 0.99\n",
    "\n",
    "# Output the model\n",
    "model_file = \"batch_\"+str(batch_size)+\"_pong.model\"\n",
    "file_handler = logging.FileHandler(str(batch_size)+\".log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "def to_tensor(I):\n",
    "    return torch.Tensor(I).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We’ll put everything in a while block for now but in reality you might set up a break condition to stop the process.\n",
    "\n",
    "The first step to our algorithm is **preprocessing** the image of the game that OpenAI Gym passed to us. \n",
    "\n",
    "In the env.wrapper code, we convert the image format from RGB to GREY because the color is not important to us and GREY format is easy to train. In addition, we resize the image from 210x160x3 to 1x84x84.\n",
    "\n",
    "The following cell may run very long time , so just be patient, or use the pre trained model to test.\n",
    "**Uncomment the \"break\" to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a tuple (s,a,r,s') and keeps a list, returns a random batch of tuples as needed\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "# Set environment that skip over the 4 observations over all the time steps\n",
    "env = make_atari(env_id)\n",
    "# Configure environment for DeepMind-style Atari\n",
    "env = wrap_deepmind(env)\n",
    "# Set the environment that can be used in pytorch\n",
    "env = wrap_pytorch(env)\n",
    "model = Pong(env.action_space.n)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "# Train step\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Set the initial exploration rate\n",
    "epsilon = epsilon_begin\n",
    "epsilon_decay = 30000\n",
    "# Decay exploration from 1 to 0.02\n",
    "import math\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_end + (epsilon_begin - epsilon_end) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "frame_seq = 0\n",
    "round_reward = 0\n",
    "# while True:\n",
    "for _ in range(2000):\n",
    "    # break\n",
    "    # reset environment\n",
    "    state = env.reset()\n",
    "    # state = pre_process(state)\n",
    "    # state = np.concatenate([state, state])\n",
    "\n",
    "    round_over = False\n",
    "    # Runing until the game round over\n",
    "    while not round_over:\n",
    "        # Set exploration rate\n",
    "        epsilon = epsilon_by_frame(frame_seq)\n",
    "        # behave according to an epsilon greedy policy\n",
    "        action = env.action_space.sample()\n",
    "        if random.random() > epsilon:\n",
    "            action = model.action(state)\n",
    "        # agent takes the action, and the environment responds\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        round_over = done\n",
    "        # next_state = pre_process(next_state)\n",
    "        # next_state = np.concatenate([state[80:], next_state])\n",
    "\n",
    "        # push to replay buffer\n",
    "        s = np.expand_dims(state, 0)\n",
    "        ns = np.expand_dims(next_state, 0)\n",
    "        replay_buffer.append((s, action, reward, ns, done))\n",
    "        # update the state\n",
    "        state = next_state\n",
    "        frame_seq += 1\n",
    "        # print(frame_seq)\n",
    "        # Gather the rewards\n",
    "        round_reward += reward\n",
    "        # Start training when there are enough memory exists\n",
    "        if len(replay_buffer) > replay_initial:\n",
    "            train()\n",
    "\n",
    "        if frame_seq % 10000 == 0:\n",
    "            # print(\"saving model\", frame_seq)\n",
    "            logger.info(\"current round: \"+str(round)+\"saving model\")\n",
    "            # from datetime import datetime\n",
    "            # start = datetime.now()\n",
    "            torch.save(model, model_file)\n",
    "            # print(datetime.now() - start)\n",
    "    # Caculate round number\n",
    "    round += 1\n",
    "    # print(\"round over\", round_reward)\n",
    "    logger.info(\"round reward: \"+str(round_reward))\n",
    "    round_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a script to save the model in the file and log rewards from time to time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log trail\n",
    "def read_log(filename):\n",
    "    results = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"round reward\"):\n",
    "                reward = line.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"round reward: \", \"\")\n",
    "                results.append(float(reward))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.plot(read_log(\"32.log\"))  # reward of the train process\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('total reward per round')\n",
    "plt.title('DQN Pong game q-learning (training)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "We can see that the agent has learned some tricks to defend and even win. From the above figures, we can figure out that at the beginning, the total rewards of the agent in one episode are around -20, which means the agent is losing the game. The reason is that with the relatively large epsilon, the agent is likely to explore more possibilities. However, with the increase of the number of game round, the rewards become larger. After around 1750 steps for batch size 32, the rewards turn positive and even reach 20, which means the agent begins to win. It can be explained that with the decay of the epsilon, the agent is prone to exploit rather than explore, leading to the higher rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the model we trained to see whether it can beat the hard coded AI or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "pong = torch.load(\"batch_32_pong.model\")\n",
    "# Set test rounds\n",
    "test_rounds = 100\n",
    "win_num = 0\n",
    "score = []\n",
    "current = 0\n",
    "for round in range(test_rounds):\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    current = 0\n",
    "    round_over = False\n",
    "    # Running until the round over\n",
    "    while not round_over:\n",
    "#         env.render()\n",
    "        # Running greedy \n",
    "        action = pong.action(state)\n",
    "        # agent takes the action, and the environment responds\n",
    "        next_state, reward, round_over, _ = env.step(action)\n",
    "        # Caculate round reward\n",
    "        current += reward\n",
    "        # Update state\n",
    "        state = next_state\n",
    "    # Calculate number of victory\n",
    "    if reward > 0:\n",
    "        win_num+=1\n",
    "    score.append(current)\n",
    "# Plot the result\n",
    "plt.plot(score)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('total reward per round')\n",
    "plt.title('DQN Pong game q-learning (test)')\n",
    "plt.ylim((-21,21))\n",
    "plt.show()\n",
    "# Print the wining rate\n",
    "print(\"we win at rate \", win_num/test_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Now, we can see that the total reward per round are all above 0, which means we win the game every time. However, the mean reward is around 15, which means we cannot hit the ball back every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us view the game\n",
    "# Play the mp4 file\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('test.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Limitations\n",
    "\n",
    "**1. The training process is too long.** With GPU, it takes us a whole day to train the model.\n",
    "\n",
    "**2. The score result is not as high as we expected.** The mean score of our model does not reach 21, which means the trained agent does not always hit the ball back every time. \n",
    "\n",
    "#Next steps\n",
    "Here are some ideas to improve the training process and agents:\n",
    "\n",
    "**1.Generate simulations of batches in parallel. ** I tried using multithreading to generate and play batches of games at the same time to increase the speed of training process. The problem was that the backend library for the game had issues creating environments in multiple threads. Maybe you can overcome this by implementing a multiprocess method.\n",
    "\n",
    "**2.Giving a reward to the agent for catching the ball could be good **for not losing (for example a +0.5 reward). Now we are just giving positive reward to agent for scoring. If it catches the ball and then gets a -1 reward, it will think it is probably bad to catch the ball. \n",
    "\n",
    "\n",
    "**3. Use a different gamma to train the model.** In our model, we use 0.99 as gamma. We may try other values for the gamma parameter, for example, 0.9. This may improve the training speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        Playing Atari with Deep Reinforcement Learning\n",
    "    </li>\n",
    "    <li>\n",
    "        ADVERSARIAL POLICIES: ATTACKING DEEP REINFORCEMENT LEARNING\n",
    "    </li>\n",
    "    Also in case you are interested in RL and DRL, these courses may help you.\n",
    "    <li> <a href=\"https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver\">Reinforcement Learning By David Silver</a></li>\n",
    "    <li> <a href = \"http://rail.eecs.berkeley.edu/deeprlcourse/\"> Deep Reinforcement Learning By Sergey Levine</a></li>\n",
    "    <li> <a href = \"https://github.com/xs2315/Deep-reinforcement-learning-for-Pong-game\"> Pong DQN </a> </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
